{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_arrays_dicts(dict_a, dict_b):\n",
    "    \"\"\"\n",
    "    Adds the numpy arrays from dict_b to the numpy arrays in dict_a for matching keys.\n",
    "    This function concatenates the array from dict_b (with 4 elements) to the array in dict_a (with 142 elements),\n",
    "    resulting in an array of 146 elements.\n",
    "\n",
    "    Parameters:\n",
    "    dict_a (dict): The first dictionary containing numpy arrays.\n",
    "    dict_b (dict): The second dictionary containing numpy arrays to add.\n",
    "\n",
    "    Returns:\n",
    "    dict: A new dictionary with the concatenated numpy arrays.\n",
    "    \"\"\"\n",
    "    # Ensure both dictionaries have the same keys\n",
    "    if dict_a.keys() != dict_b.keys():\n",
    "        raise ValueError(\"The dictionaries do not have the same keys.\")\n",
    "    \n",
    "    # Initialize an empty dictionary to store the results\n",
    "    result_dict = {}\n",
    "\n",
    "    # Iterate over each key\n",
    "    for key in dict_a:\n",
    "        # Ensure both dictionaries have lists of numpy arrays for the current key\n",
    "        if len(dict_a[key]) != len(dict_b[key]):\n",
    "            raise ValueError(f\"Length mismatch for key '{key}' in both dictionaries.\")\n",
    "        \n",
    "        # Concatenate corresponding numpy arrays from dict_a and dict_b\n",
    "        result_dict[key] = [np.concatenate((a, b)) for a, b in zip(dict_a[key], dict_b[key])]\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def add_last_n_elements(dict_a, dict_b, n):\n",
    "    \"\"\"\n",
    "    Adds the last 'n' elements of the numpy arrays from dict_b to the numpy arrays in dict_a for matching keys.\n",
    "    This function concatenates the last 'n' elements from dict_b (with 'n' elements) to the array in dict_a (with 'm' elements),\n",
    "    resulting in an array of 'm + n' elements.\n",
    "\n",
    "    Parameters:\n",
    "    dict_a (dict): The first dictionary containing numpy arrays.\n",
    "    dict_b (dict): The second dictionary containing numpy arrays to add.\n",
    "    n (int): The number of elements to add from each array in dict_b.\n",
    "\n",
    "    Returns:\n",
    "    dict: A new dictionary with the concatenated numpy arrays.\n",
    "    \"\"\"\n",
    "    # Ensure both dictionaries have the same keys\n",
    "    if dict_a.keys() != dict_b.keys():\n",
    "        raise ValueError(\"The dictionaries do not have the same keys.\")\n",
    "    \n",
    "    # Initialize an empty dictionary to store the results\n",
    "    result_dict = {}\n",
    "\n",
    "    # Iterate over each key\n",
    "    for key in dict_a:\n",
    "        # Ensure both dictionaries have lists of numpy arrays for the current key\n",
    "        if len(dict_a[key]) != len(dict_b[key]):\n",
    "            raise ValueError(f\"Length mismatch for key '{key}' in both dictionaries.\")\n",
    "        \n",
    "        # Concatenate the numpy array from dict_a with the last n elements from the corresponding array in dict_b\n",
    "        result_dict[key] = [\n",
    "            np.concatenate((a, b[-n:])) for a, b in zip(dict_a[key], dict_b[key])\n",
    "        ]\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../data/extracted_features_v2/mfcc_20_features.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m original_set_path   \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../../data/extracted_features_v2/mfcc_20_features.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m additional_set_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../../data/extracted_features_v2/format_frequencies.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(original_set_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m     original_feature_set \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(additional_set_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../data/extracted_features_v2/mfcc_20_features.pickle'"
     ]
    }
   ],
   "source": [
    "original_set_path   = \"../../../data/extracted_features_v2/mfcc_20_features.pickle\"\n",
    "additional_set_path = \"../../../data/extracted_features_v2/format_frequencies.pickle\"\n",
    "\n",
    "with open(original_set_path, \"rb\") as file:\n",
    "    original_feature_set = pickle.load(file)\n",
    "\n",
    "with open(additional_set_path, \"rb\") as file:\n",
    "    additional_feature_set = pickle.load(file)\n",
    "\n",
    "print(f\"original feature set\")\n",
    "print(f\"    number of samples : {len(original_feature_set['19'])}\")\n",
    "print(f\"    number of features: {len(original_feature_set['19'][0])}\")\n",
    "\n",
    "print(f\"additional feature set\")\n",
    "print(f\"    number of samples : {len(additional_feature_set['19'])}\")\n",
    "print(f\"    number of features: {len(additional_feature_set['19'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new feature set\n",
      "    number of samples : 300\n",
      "    number of features: 146\n",
      "1st original: -345.1817626953125\n",
      "1st new     : -345.1817626953125\n",
      "last original: 468.4084049203264\n",
      "additional   : [1.75809862e+03 1.81810197e+03 2.92269196e-02 3.68823242e+03]\n",
      "last 5 new   : [4.68408405e+02 1.75809862e+03 1.81810197e+03 2.92269196e-02\n",
      " 3.68823242e+03] \n"
     ]
    }
   ],
   "source": [
    "new_feature_set = add_arrays_dicts(original_feature_set, additional_feature_set)\n",
    "\n",
    "print(f\"new feature set\")\n",
    "print(f\"    number of samples : {len(new_feature_set['19'])}\")\n",
    "print(f\"    number of features: {len(new_feature_set['19'][0])}\")\n",
    "\n",
    "\n",
    "print(f\"1st original: {original_feature_set['19'][0][0]}\")\n",
    "print(f\"1st new     : {new_feature_set['19'][0][0]}\")\n",
    "\n",
    "print(f\"last original: {original_feature_set['19'][0][-1]}\")\n",
    "print(f\"additional   : {additional_feature_set['19'][0]}\")\n",
    "print(f\"last 5 new   : {new_feature_set['19'][0][-5:]} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../../data/extracted_features_v2/mfcc_20_format_freq.pickle\", \"wb\") as file:\n",
    "    pickle.dump(new_feature_set, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original feature set\n",
      "    number of samples : 300\n",
      "    number of features: 140\n",
      "additional feature set\n",
      "    number of samples : 300\n",
      "    number of features: 142\n"
     ]
    }
   ],
   "source": [
    "original_set_path   = \"../../../data/extracted_features_v2/lfcc_features.pickle\"\n",
    "additional_set_path = \"../../../data/extracted_features_v2/mfcc_20_features.pickle\"\n",
    "\n",
    "with open(original_set_path, \"rb\") as file:\n",
    "    original_feature_set = pickle.load(file)\n",
    "\n",
    "with open(additional_set_path, \"rb\") as file:\n",
    "    additional_feature_set = pickle.load(file)\n",
    "\n",
    "print(f\"original feature set\")\n",
    "print(f\"    number of samples : {len(original_feature_set['19'])}\")\n",
    "print(f\"    number of features: {len(original_feature_set['19'][0])}\")\n",
    "\n",
    "print(f\"additional feature set\")\n",
    "print(f\"    number of samples : {len(additional_feature_set['19'])}\")\n",
    "print(f\"    number of features: {len(additional_feature_set['19'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new feature set\n",
      "    number of samples : 300\n",
      "    number of features: 142\n",
      "1st original: 6.589507102966309\n",
      "1st new     : 6.589507102966309\n",
      "last original: -2.663179397583008\n",
      "additional   : [514.873637   468.40840492]\n",
      "last 5 new   : [ -3.66989946  -3.18244052  -2.6631794  514.873637   468.40840492] \n"
     ]
    }
   ],
   "source": [
    "new_feature_set = add_last_n_elements(original_feature_set, additional_feature_set, 2)\n",
    "\n",
    "print(f\"new feature set\")\n",
    "print(f\"    number of samples : {len(new_feature_set['19'])}\")\n",
    "print(f\"    number of features: {len(new_feature_set['19'][0])}\")\n",
    "\n",
    "\n",
    "print(f\"1st original: {original_feature_set['19'][0][0]}\")\n",
    "print(f\"1st new     : {new_feature_set['19'][0][0]}\")\n",
    "\n",
    "print(f\"last original: {original_feature_set['19'][0][-1]}\")\n",
    "print(f\"additional   : {additional_feature_set['19'][0][-2:]}\")\n",
    "print(f\"last 5 new   : {new_feature_set['19'][0][-5:]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../../data/extracted_features_v2/lfcc_20_pitch.pickle\", \"wb\") as file:\n",
    "    pickle.dump(new_feature_set, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
