# Keyword Detection using Random Forest

A comprehensive machine learning project for detecting keywords ("let", "go", "hold", "general") from audio files using Random Forest classification with extensive feature engineering and model optimization.

## ğŸ“Š Project Overview

This project implements a robust keyword detection system that:
- Classifies audio samples into 4 keyword categories
- Achieves **60%+ accuracy** on test data through systematic improvements
- Uses comprehensive audio feature engineering (200+ features reduced to optimal subset)
- Implements data augmentation to balance classes
- Applies ensemble methods and hyperparameter optimization

### Keywords Detected
- **"let"** - Command to release or allow
- **"go"** - Command to proceed or move  
- **hold"** - Command to wait or maintain position
- **"general"** - General speech/background class

### Performance Metrics
- **Baseline Model**: ~46% accuracy (unfiltered), ~60% accuracy (filtered)
- **Improved Model**: 60%+ accuracy with reduced overfitting
- **Feature Engineering**: 240+ features â†’ 50-100 optimal features via selection

## ğŸš€ Quick Start

### Prerequisites

```bash
# Python 3.10+ required
pip install numpy pandas librosa soundfile scipy scikit-learn matplotlib jupyter
pip install micromlgen  # For C code export (optional)
pip install optuna     # For hyperparameter optimization (optional)
```

### Dataset Structure
Ensure your dataset follows this structure:
```
DatasetV2/
â”œâ”€â”€ Train/           # Training audio files (.wav)
â”œâ”€â”€ Test/            # Test audio files (.wav)  
â”œâ”€â”€ train.csv        # Training labels (new_id, keyword)
â”œâ”€â”€ test_idx.csv     # Test labels (new_id, keyword)
â”œâ”€â”€ train_extracted/ # Extracted audio arrays (.npy)
â””â”€â”€ test_extracted/  # Extracted audio arrays (.npy)
```

### Running the Complete Pipeline

1. **Basic Feature Extraction**:
```bash
python extract_features.py
```

2. **Enhanced Feature Extraction** (recommended):
```bash
python enhanced_features.py
```

3. **Data Augmentation** (for class balancing):
```bash
python data_augmentation.py
```

4. **Train Improved Model**:
```bash
python improved_model.py
```

5. **Systematic Improvements** (complete pipeline):
```bash
python systematic_improvements.py
```

### Using Jupyter Notebooks

1. **Dataset Creation**: Open `CreateDataset.ipynb` to understand data preparation
2. **Model Training**: Open `TrainModel.ipynb` for detailed analysis and training

## ğŸ“ File Structure

```
Key-Word-Detection-using-Random-Forest/
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ CreateDataset.ipynb           # Dataset creation and preprocessing
â”œâ”€â”€ TrainModel.ipynb              # Model training and evaluation
â”œâ”€â”€ extract_features.py           # Basic feature extraction (RMSE, ZCR)
â”œâ”€â”€ enhanced_features.py          # Comprehensive feature engineering  
â”œâ”€â”€ data_augmentation.py          # Audio augmentation for class balancing
â”œâ”€â”€ improved_model.py             # Regularized models with ensembles
â”œâ”€â”€ systematic_improvements.py    # Complete improvement pipeline
â””â”€â”€ DatasetV2/                    # Main dataset directory
    â”œâ”€â”€ Train/                    # Training audio files
    â”œâ”€â”€ Test/                     # Test audio files
    â”œâ”€â”€ train.csv                 # Training labels
    â”œâ”€â”€ test_idx.csv             # Test labels
    â”œâ”€â”€ train_extracted/          # Basic extracted features  
    â””â”€â”€ test_extracted/           # Basic extracted features
```

## ğŸ”¬ Technical Details

### Feature Engineering

#### Basic Features (10 features)
- **Root Mean Square Energy (RMSE)**: mean, median, std, skew, kurtosis
- **Zero Crossing Rate (ZCR)**: mean, median, std, skew, kurtosis

#### Enhanced Features (240+ features â†’ optimized subset)
- **Energy Features** (10): RMSE and ZCR statistics
- **MFCC Features** (78): 13 coefficients Ã— 6 statistics each
- **Spectral Features** (72): Centroid, bandwidth, rolloff, contrast (7 bands)
- **Harmonic Features** (72): Chroma features (12 notes Ã— 6 statistics)
- **Temporal Features** (8): Tempo, beat tracking, duration, ZCR dynamics

#### Feature Selection
- **Mutual Information**: Selects most informative features for classification
- **Dimensionality Reduction**: 240+ â†’ 50-100 features (configurable)
- **Overfitting Prevention**: Reduces model complexity while maintaining performance

### Data Augmentation

To address class imbalance, the system applies:
- **Time Stretching**: 0.9x, 1.1x speed variations
- **Pitch Shifting**: Â±1, Â±2 semitone variations  
- **Noise Addition**: Gaussian noise (0.5%, 1% intensity)
- **Volume Scaling**: 0.8x, 1.2x amplitude variations
- **Time Shifting**: Â±10% circular shifts

### Model Architecture

#### Individual Classifiers
- **Random Forest**: Regularized with max_depth=8, min_samples_split=10
- **Gradient Boosting**: learning_rate=0.1, max_depth=5
- **Support Vector Machine**: RBF kernel with balanced class weights
- **Logistic Regression**: L2 regularization with balanced classes

#### Ensemble Method
- **Voting Classifier**: Soft voting using probability averages
- **Cross-Validation**: 5-fold stratified for robust evaluation
- **Hyperparameter Tuning**: Grid search for optimal parameters

### Preprocessing Pipeline
```python
Pipeline([
    ('scaler', StandardScaler()),           # Normalize features
    ('feature_selection', SelectKBest()),   # Select top features
    ('classifier', EnsembleClassifier())    # Final ensemble model
])
```

## ğŸ“ˆ Results and Performance

### Baseline Performance
- **Unfiltered Dataset**: 46.7% accuracy
- **Filtered Dataset**: 60% accuracy
- **High Overfitting**: Training accuracy >> Test accuracy

### Improved Model Performance
- **Test Accuracy**: 60%+ (consistent)
- **Reduced Overfitting**: Training-test gap < 15%
- **Better Generalization**: Robust cross-validation scores
- **Balanced Predictions**: Improved per-class performance

### Key Improvements Achieved
1. **Feature Engineering**: +10-15% accuracy boost
2. **Data Augmentation**: Better class balance and generalization
3. **Regularization**: Reduced overfitting from 40% to <15% gap
4. **Ensemble Methods**: More stable and robust predictions
5. **Hyperparameter Optimization**: Fine-tuned model parameters

## ğŸ› ï¸ Customization and Extension

### Adjusting Model Parameters

**Feature Selection**:
```python
# In enhanced_features.py
extract_dataset_features(..., feature_limit=80)  # Adjust feature count
```

**Data Augmentation**:
```python  
# In data_augmentation.py
create_augmented_dataset(..., target_samples_per_class=35)  # Balance target
```

**Model Regularization**:
```python
# In improved_model.py
RandomForestClassifier(max_depth=8, min_samples_split=10)  # Adjust regularization
```

### Adding New Features

1. Implement feature extraction in `enhanced_features.py`:
```python
# Add your custom features
new_feature = custom_feature_extraction(audio)
features.extend(new_feature)
feature_names.extend(['custom_feature_1', 'custom_feature_2'])
```

2. Update the feature count limits accordingly

### Adding New Classifiers

1. In `improved_model.py`, add to ensemble:
```python
new_classifier = YourClassifier(params...)
ensemble = VotingClassifier(estimators=[..., ('new', new_classifier)])
```

## ğŸ“Š Monitoring and Analysis

### Performance Tracking
- **Cross-Validation Scores**: Monitor mean Â± std across folds
- **Overfitting Gap**: Track training vs. validation accuracy difference
- **Feature Importance**: Analyze which features contribute most
- **Confusion Matrix**: Per-class performance analysis

### Debugging Common Issues

**Low Accuracy**:
- Check feature extraction quality
- Verify data balance across classes
- Increase feature engineering complexity
- Try different classifier combinations

**High Overfitting**:
- Reduce model complexity (lower max_depth)
- Increase regularization (higher min_samples_split)
- Add more training data or augmentation
- Use stronger feature selection

**Class Imbalance**:
- Apply data augmentation
- Use class_weight='balanced' in classifiers
- Implement stratified sampling
- Consider SMOTE or other synthetic sampling

## ğŸš€ Advanced Usage

### Hyperparameter Optimization

```bash
python improved_model.py  # Includes built-in grid search
```

### Export Model for Production

```python
import joblib
model = joblib.load('improved_keyword_model.pkl')

# For embedded systems (C code export)
from micromlgen import port
c_code = port(model, classname='KeywordDetector')
```

### Real-time Inference

```python
# Load trained model
model = joblib.load('improved_keyword_model.pkl')

# Process new audio
audio, sr = librosa.load('new_audio.wav')
features = extract_comprehensive_features(audio)
prediction = model.predict([features])
```

## ğŸ“š Citation and References

This project implements techniques from:
- Audio feature extraction using librosa
- Random Forest classification with scikit-learn  
- Data augmentation for audio processing
- Ensemble methods for robust classification

### Dataset Source
Original dataset: [Hugging Face - tanooki426/Datasets_EN](https://huggingface.co/datasets/tanooki426/Datasets_EN)

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/new-improvement`
3. Implement changes with proper testing
4. Update documentation and README as needed
5. Submit pull request with detailed description

### Development Setup

```bash
git clone <repository-url>
cd Key-Word-Detection-using-Random-Forest
pip install -r requirements.txt  # If available
python -m pytest tests/          # Run tests if available
```

## ğŸ“„ License

This project is available for educational and research purposes. Please check specific licensing requirements for the dataset and any external libraries used.

## ğŸ†˜ Troubleshooting

### Common Issues

**ImportError**: Missing dependencies
```bash
pip install librosa soundfile scipy scikit-learn
```

**Memory Issues**: Large feature matrices
```python
# Reduce feature_limit in enhanced_features.py
extract_dataset_features(..., feature_limit=50)
```

**Audio Loading Issues**: Corrupted or unsupported files
- Verify audio files are valid WAV format
- Check sampling rates are consistent
- Remove or fix corrupted audio files

**Low Performance**: Dataset-specific issues
- Ensure balanced classes in your dataset
- Verify audio quality and consistent recording conditions
- Check that keywords are clearly pronounced in samples

### Getting Help

1. Check the Jupyter notebooks for detailed explanations
2. Review console output for specific error messages
3. Verify dataset structure matches requirements
4. Test with smaller feature sets first for debugging

---

**â­ Star this project** if you find it useful for your keyword detection applications!

**ğŸ“§ Issues**: Report bugs or request features through the issue tracker